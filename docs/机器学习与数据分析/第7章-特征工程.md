# 第 7 章：特征工程：让数据“开口说话”

业界流传着一句至理名言：

> **“数据和特征决定了机器学习的上限，而模型和算法只是在逼近这个上限。”**

在前面的章节中，我们的数据往往是现成的数字。但在现实世界中，数据是杂乱无章的：有文本（“北京”、“上海”）、有量纲差异巨大的数字（身高 1.8米 vs 工资 20000元）、甚至有非线性的关系。

本章我们将学习特征工程的 **“三大战役”**，把原始数据改造成模型最爱吃的样子。

---

## 7.1 第一战：类别编码 (Categorical Encoding) —— 翻译官

AI 模型本质上是数学公式，它只认识数字（0, 1, 2...），不认识汉字或单词。我们需要把文字“翻译”成数字。

### 7.1.1 数据的两种“身份”

在翻译前，必须先判断数据的性质，否则会酿成大错：

1. **有序类别 (Ordinal):** 有等级、大小之分。
* *例子:* 尺码 (S < M < L)、学历 (本科 < 硕士 < 博士)。
* *对策:* **标签编码 (Label Encoding)**。


2. **无序类别 (Nominal):** 平等关系，无大小之分。
* *例子:* 颜色 (红/蓝/绿)、城市 (北京/上海)、性别 (男/女)。
* *对策:* **独热编码 (One-Hot Encoding)**。



### 7.1.2 战术 A：标签编码 (Label Encoding)

对于有序数据，我们可以直接将其映射为整数：。

```python
import pandas as pd

# 1. 模拟衣服尺码数据
df = pd.DataFrame({'尺码': ['S', 'XL', 'M', 'L', 'S', 'M']})

# 2. 自定义映射字典 (必须人为指定顺序，否则机器不知道 S 和 L 谁大)
size_mapping = {
    'S': 1,
    'M': 2,
    'L': 3,
    'XL': 4
}

# 3. 翻译
df['尺码_Code'] = df['尺码'].map(size_mapping)
print(df)

```

*结果:* S 变成了 1，XL 变成了 4。模型能理解 ，逻辑正确。

### 7.1.3 战术 B：独热编码 (One-Hot Encoding)

对于“城市”这种无序数据，如果我们强行标记为 `北京=1, 上海=2`，模型会错误地认为 `上海 > 北京`。
**独热编码**的逻辑是：有多少种分类，就新建多少个“开关”列。

```python
# 1. 模拟城市数据
df_city = pd.DataFrame({'城市': ['北京', '上海', '深圳', '北京']})

# 2. 独热编码 (Pandas 神器 get_dummies)
# prefix: 新列名的前缀
df_onehot = pd.get_dummies(df_city, columns=['城市'], prefix='City')

# 3. 转换为整数 (True/False -> 1/0)
df_onehot = df_onehot.astype(int)
print(df_onehot)

```

*结果:*
数据变成了三列：`City_北京`, `City_上海`, `City_深圳`。如果是北京，则 `City_北京` 为 1，其余为 0。

> **⚠️ 进阶警示 (维度爆炸):** 如果你的类别特别多（如 1 万个商品 ID），使用 One-Hot 会产生 1 万列特征，导致计算崩溃。此时需要使用更高级的 Target Encoding 或 Embedding（深度学习）。

---

## 7.2 第二战：特征缩放 (Feature Scaling) —— 统一度量衡

这是新手最容易忽略的步骤，也是**导致模型准确率低**的头号杀手。

### 7.2.1 为什么要缩放？

假设我们要预测房屋价值，有两个特征：

* **面积:** 50 ~ 200 (平米)
* **房间数:** 1 ~ 5 (个)

在 KNN（计算距离）或 神经网络（梯度下降）眼中，**面积的变化幅度（几百）远大于房间数（个位数）**。
**后果:** 模型会认为“面积”重要一万倍，完全忽略“房间数”的存在。

### 7.2.2 战术 A：标准化 (Standardization / Z-Score) **(⭐强烈推荐)**

将数据转换为 **均值为 0，标准差为 1** 的分布。


* **适用:** 绝大多数算法（SVM, 逻辑回归, 线性回归, 神经网络）。
* **特点:** 不容易受异常值影响。

### 7.2.3 战术 B：归一化 (Min-Max Scaling)

将数据强行压缩到 `[0, 1]` 区间。


* **适用:** 对范围有严格要求的场景（如图像像素 0-255）。
* **缺点:** 如果有一个异常大的值，其他正常值都会被压缩到 0 附近。

### 💻 代码实战

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import numpy as np

# 模拟数据：两个特征量纲差距巨大
X = np.array([
    [50, 1],   # 小房子
    [200, 5],  # 大别墅
    [100, 2]   # 中等房
])

# 1. 实例化标准化器
scaler = StandardScaler()

# 2. 训练并转换
# 注意：fit_transform = 先计算均值方差(fit)，再执行转换(transform)
X_scaled = scaler.fit_transform(X)

print("原始数据:\n", X)
print("\n缩放后 (Z-Score):\n", X_scaled)

```

*结果:* 所有数据都变成了 -1 到 1 之间的小数，模型可以公平对待了。

> **🚨 黄金法则:** 在做机器学习项目时，**必须**遵循：
> 1. 在训练集上 `fit` (计算均值)。
> 2. 在训练集上 `transform`。
> 3. 在测试集上 **只做** `transform` (**绝对不能在测试集上 fit！** 否则就是作弊，因为你偷看了测试集的分布)。
> 
> 

---

## 7.3 第三战：特征变换与构造 —— 挖掘机

有时候，原始数据不够好，我们需要手动“捏”出新特征。

### 7.3.1 数据分箱 (Binning): 处理非线性

有些特征和结果不是线性的。比如“年龄”与“健康风险”通常是 **U 型曲线**（幼年和老年风险高，青壮年风险低）。
线性模型画不出 U 型线，我们可以把年龄切成段（少年、青年、老年），变成类别特征。

```python
# Pandas cut 分箱
df['年龄段'] = pd.cut(df['年龄'], bins=[0, 18, 50, 100], labels=['少年', '青年', '老年'])
# 然后再做 One-Hot 编码

```

### 7.3.2 特征构造 (Feature Interaction): 创造新知识

模型是很笨的。如果你给它“长”和“宽”，它不一定知道“面积”就是长乘以宽。我们需要帮它造出来。

* **多项式特征 (Polynomial Features):** 自动生成  等特征。

```python
from sklearn.preprocessing import PolynomialFeatures

# 原始特征: [长, 宽]
X = [[2, 3], [4, 5]]

# 构造 2 次多项式
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# 结果特征变成了 5 个: [长, 宽, 长^2, 长*宽, 宽^2]
# 其中 "长*宽" 就是面积！这一步极大提升了模型的表现。

```

---

## 本章总结：特征工程检查清单

在把数据喂给模型之前，请按顺序检查以下事项：

1. **翻译 (Encoding):**
* 有序文字  Label Encoding
* 无序文字  One-Hot Encoding


2. **度量 (Scaling):**
* 所有数字特征  StandardScaler (标准化)


3. **挖掘 (Construction):**
* 非线性关系  分箱 (Binning)
* 隐式关系  多项式特征/业务组合特征

