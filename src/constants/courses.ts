import { Course, Difficulty } from '../types';

export const COURSES: Course[] = [
  {
    id: 'ml-101',
    title: '机器学习基础',
    description: '探索经典算法，从线性回归到决策树，构建你的AI基石。',
    icon: 'BrainCircuit',
    color: 'from-cyan-500 to-blue-600',
    difficulty: Difficulty.Beginner,
    lessons: [
      {
        id: 'ml-00-intro',
        title: '开启 AI 之旅：技能树与路线图',
        description: '了解 AI 学习路线、必备数学基础与编程技能，避开学习路上的“绝望之谷”。',
        content: `
# 第 0 章：开启 AI 之旅：技能树与路线图

欢迎来到人工智能（AI）的世界。在敲下第一行代码之前，我们需要先调整一下“导航仪”，看看我们将要去哪里，背包里需要带什么，以及路上会遇到什么样的风景和挑战。

## 0.1 我们为什么学 AI？

学习 AI 不仅仅是为了追逐热点，更是为了掌握一种全新的**解决问题的范式**。

> [!TIP]
> **Paradigm Shift:** 
> 传统的 Software 1.0 是你告诉电脑“怎么做”；而 Software 2.0 (AI) 是你告诉电脑“想要什么”，让它自己学会怎么做。

- **从 Software 1.0 到 Software 2.0:**
  - **传统编程 (1.0):** 程序员显式地编写规则（If-Else）。比如写一个垃圾邮件过滤器，你需要列出所有可能的敏感词。这很难穷尽。
  - **AI 编程 (2.0):** 程序员不再编写规则，而是**定义目标**并提供**数据**，让算法自己去寻找规则。你只需要给模型看 1 万封垃圾邮件和 1 万封正常邮件，它就能自己学会分辨。

\`\`\`python
# Software 2.0 的伪代码
def train_model(data, labels):
    # 机器自己寻找 pattern
    model = Model()
    for epoch in range(100):
        predictions = model(data)
        error = loss(predictions, labels)
        model.update(error)
    return model
\`\`\`

## 0.2 必备的数学与编程锦囊

不要被“人工智能”这个词吓倒。对于大多数应用者来说，你不需要成为数学家。请检查你的“行囊”里是否具备了以下基础装备：

### 🎒 装备包 A：数学基础 (理论核心)

数学是我们理解模型“为什么生效”以及“为什么失效”的工具。

> [!WARNING]
> 不要试图在一开始就搞懂所有推导！**先看几何意义 (Geometric Intuition)**，再看代码实现，最后才是数学证明。

1. **线性代数 (Linear Algebra) —— 数据的容器**
   - **必须掌握:** 向量、矩阵乘法、维度变换 (Reshape)。
   - **为什么:** 计算机眼中的图片不是图，而是一个巨大的数字矩阵。深度学习的本质就是大规模的矩阵运算。
2. **微积分 (Calculus) —— 优化的引擎**
   - **必须掌握:** 导数、偏导数、链式法则、梯度 (Gradient)。
   - **为什么:** 训练模型的过程就像“下山”，梯度告诉我们往哪个方向走能最快到达山谷（即误差最小点）。
3. **概率统计 (Probability) —— 不确定性的度量**
   - **必须掌握:** 分布 (高斯分布)、期望、贝叶斯定理。
   - **为什么:** 现实世界是充满噪声的，模型输出的永远是一个概率（比如：这是一只猫的概率是 98%）。

\`\`\`quiz
{
  "question": "在深度学习中，为什么我们需要计算梯度 (Gradient)？",
  "options": [
    "为了让数据变得更复杂",
    "为了找到函数下降最快的方向，从而最小化误差",
    "为了增加模型的参数量",
    "为了把矩阵转换成向量"
  ],
  "answer": 1,
  "explanation": "梯度指向函数增长最快的方向，因此它的反方向就是下降最快的方向。在训练中，我们沿着梯度的反方向更新参数，以最小化损失函数 (Loss)。"
}
\`\`\`

### 💻 装备包 B：编程基础 (工程核心)

算法最终要落地为代码。

1. **Python 语言:** AI 领域的通用语。需熟练掌握列表推导式、类 (Class) 与对象、装饰器等中级特性。
2. **科学计算三剑客 (NumPy/Pandas/Matplotlib):**
   - **NumPy:** 所有的 AI 框架底层逻辑都和它相似，它是处理多维数组的标准。
   - **Pandas:** 你的“Excel 杀手”，用于清洗和处理结构化数据。
   - **Matplotlib:** 画出 Loss 曲线，直观地看到模型的训练情况。
3. **Scikit-Learn:** 传统机器学习工具箱。提供现成的算法实现（如决策树、SVM、随机森林），是理解机器学习流程的最佳起点。
4. **深度学习框架 (建议 PyTorch):** 它是目前学术界和工业界最流行的框架，如同乐高积木一样灵活。

------

## 0.3 学习曲线：如何度过“绝望之谷”

学习 AI 并不是一条直线向上的坦途，你将经历著名的**“达克效应” (Dunning-Kruger Effect)** 曲线。提前预知这些阶段，能帮你坚持到底。

### 第一阶段：蜜月期 (The Hand-holding Phase)

- **现象:** 你跟着教程跑通了 Hello World 级别的 Demo（如手写数字识别），代码一运行，准确率 99%。
- **心态:** “AI 也不过如此嘛，简单！”（这就是**愚昧之山**的顶峰）。

### 第二阶段：绝望之谷 (The Valley of Despair) ⚠️ *高危流失区*

- **现象:** 当你试图把自己找来的数据喂给模型时，报错开始了：
  - \`RuntimeError: size mismatch\` (维度不对)
  - \`CUDA out of memory\` (显存炸了)
  - Loss 居高不下，模型什么都学不到。
- **真相:** 你发现自己并没有真正理解代码背后的原理，只会复制粘贴。
- **如何度过:**
  - **不要死磕数学推导:** 遇到不懂的公式，先看几何意义，再看代码实现，最后再看推导。
  - **学会调试:** 善用 \`print(tensor.shape)\`，90% 的 Bug 都是因为矩阵维度对不上。
  - **回归数据:** 检查你的数据是否干净，标签是否正确。

### 第三阶段：开悟之坡 (The Slope of Enlightenment)

- **现象:** 你开始理解为什么 Learning Rate 设大了会震荡，为什么这里需要加一层 Normalization。你能看懂 PyTorch 的官方文档了。
- **行动:** 你开始系统地学习经典网络结构 (ResNet, Transformer)，并尝试复现简单的论文。

### 第四阶段：精通高原 (Plateau of Productivity)

- **现象:** 工具已经不再是阻碍。你拿到一个实际问题，脑海里能迅速构建出适用的模型架构，并知道如何优化它。

------

🎓 总结:

在这个教程中，我们将尽量拉平“绝望之谷”，通过可视化的原理解析和模块化的代码实践，带你平稳地走上“开悟之坡”。
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'ml-01-env',
        title: '工欲善其事：全景式认识 AI 开发环境',
        description: 'Anaconda, Jupyter, Scikit-Learn... 构建清晰的工具地图，为机器学习做好准备。',
        content: `
# 第 1 章：工欲善其事：全景式认识 AI 开发环境

在上一章我们了解了心法（数学）和招式（编程），现在我们要走进“兵器库”。

对于初学者来说，环境配置往往是第一只拦路虎。Anaconda, Miniconda, Scikit-Learn, PyTorch... 这些名词到底是什么关系？我需要全部安装吗？

本章不涉及具体的安装命令，而是要为你构建一张**清晰的工具地图**。弄懂了这张图，后续的配置将易如反掌。

## 1.1 算力基石：CPU vs. GPU

虽然在传统的机器学习（Machine Learning, ML）阶段，普通电脑的 CPU 通常足够应付，但了解算力区别对未来进阶至关重要。

* **CPU (中央处理器):**
  * **角色:** **“老教授”**。核心少，但擅长复杂的逻辑控制。
  * **现状:** 在学习 **Scikit-Learn** 等传统机器学习算法时，CPU 是主力，处理表格数据绰绰有余。

* **GPU (图形处理器):**
  * **角色:** **“千人计算团”**。核心多，擅长并发计算。
  * **现状:** 到了 **深度学习** 阶段（处理图像、大模型），才必须用到 GPU 加速。

---

## 1.2 软件管家：Python, Anaconda 与 Miniconda

很多新手最困惑的问题是：*“我已经装了 Python，为什么还要装 Conda？Anaconda 和 Miniconda 又选哪个？”*

这就涉及到了**依赖管理**的问题。

* **原生 Python:** 就像一个**裸露的发动机**。你可以直接往上装各种配件（库），但很容易产生冲突。
* **Conda (Anaconda/Miniconda):** 就像一个**专业的 4S 店**。它的核心功能是 **虚拟环境 (Virtual Environment)**，允许你创建多个相互隔离的空间，防止不同项目的环境打架。

### ❓ 选择困难症：Anaconda vs. Miniconda

| 特性 | **Anaconda** (巨无霸) | **Miniconda** (精简版) |
| --- | --- | --- |
| **安装包大小** | 很大 (500MB+) | 很小 (50MB左右) |
| **预装库** | 预装了 NumPy, Pandas 等 1500+ 个科学计算包 | 只有 Python 和 Conda 基础管理器 |
| **比喻** | **精装修豪宅**：拎包入住，家具齐全，但有很多你永远不用的东西占地方。 | **毛坯房**：只给你四面墙，你需要什么家具（库）自己买（安装），完全定制化。 |
| **推荐人群** | **初学者** (硬盘空间充足，不想折腾安装包) | **进阶用户** (想要清爽环境，或者服务器部署) |

> **✅ 教程建议:** 为了减少“缺包”带来的报错，本教程初期推荐使用 **Anaconda**；如果你对电脑洁癖较重，**Miniconda** 也是极好的选择。

---

## 1.3 作战平台：Jupyter Notebook (实验室)

代码写在哪里？这取决于你的开发阶段。

* **Jupyter Notebook:**
  * **形态:** 网页版的交互式笔记本。
  * **特点:** 代码分块执行，**所见即所得**。运行一段代码，立刻就能在下方看到数据图表。
  * **地位:** 数据分析与机器学习入门的**绝对首选**。

* **IDE (PyCharm / VS Code):**
  * **地位:** 适合工程化部署。初学阶段如果觉得设置太复杂，可暂时略过。

---

## 1.4 核心武器库：Scikit-Learn (本阶段重点)

在机器学习阶段，我们暂时不需要动用“重型火炮”，一把瑞士军刀足矣。

### 1. 主力武器：Scikit-Learn (sklearn)

这是 Python 生态中最著名、最系统的**传统机器学习库**。

* **地位:** 就像是 AI 界的“标准教科书”。
* **能做什么:** 它封装了几乎所有经典的机器学习算法。
  * **分类:** 识别邮件是垃圾邮件还是正常邮件 (SVM, 决策树)。
  * **回归:** 预测房价走势 (线性回归, 随机森林)。
  * **聚类:** 将用户按消费习惯分组 (K-Means)。
  * **预处理:** 数据的归一化、降维 (PCA)。

* **为什么先学它:** 它的 API 设计极其统一且优雅（所有算法都是 \`.fit()\` 训练，\`.predict()\` 预测），是你理解机器学习流程的最佳入口。

### 2. 重型火炮：PyTorch / TensorFlow (深度学习)

* **地位:** 也就是大家常说的“AI 框架”。
* **能做什么:** 搭建神经网络，处理图像、自然语言等非结构化数据。
* **本阶段策略:** 在掌握了 Scikit-Learn 的基本概念后，我们会在后续的进阶章节再引入它们。

---

## 1.5 “云端”借力：算力平台推荐

如果你不想在本地配置环境，或者电脑配置较低，可以使用云端算力平台。

### 1. [AutoDL](https://www.autodl.com/) (国内首选)

* **特点:** 专为 AI 开发者设计的算力租赁平台。
* **优势:** 服务器在国内，**网速快，无需特殊网络环境**。环境镜像丰富（一键加载配置好的 PyTorch/TensorFlow 镜像）。
* **价格:** 性价比极高，适合学生党和个人开发者（按小时计费，用完即停）。

### 2. [Google Colab](https://colab.research.google.com/) (谷歌全家桶)

* **特点:** 谷歌提供的免费 Jupyter Notebook 环境。
* **优势:** 直接赠送免费的 GPU（甚至 TPU）算力，与 Google Drive 无缝打通。
* **注意:** 需要具备科学上网环境，且免费版有使用时长限制（容易断连）。

### 3. [Kaggle Kernels](https://www.kaggle.com/code) (数据竞赛社区)

* **特点:** 全球最大的数据科学社区提供的在线环境。
* **优势:** 平台上有海量的公开数据集，无需下载即可直接挂载到代码中跑，非常适合练手。
* **注意:** 同样需要一定的网络环境支持。

---

## 本章小结：你的入门装备

通过本章，我们确定了第一阶段的学习装备：

* **开发环境:** Anaconda (或 Miniconda)
* **编程工具:** Jupyter Notebook
* **核心算法库:** **Scikit-Learn** (重点攻克对象)
* **数据处理:** NumPy & Pandas (必不可少的助手)

---
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'ml-02-devtools',
        title: '现代工坊：VS Code + Jupyter',
        description: '掌握现代 AI 开发者的标准工作流：VS Code、Jupyter、Anaconda 三位一体。',
        content: `
# 第 2 章：现代工坊：VS Code + Jupyter 的数据炼金术

在上一章我们认识了工具，现在我们要把它们组装起来。

虽然 Jupyter 原生界面是网页版的，但在实际工作中，我们更推荐使用 **VS Code** 来运行 Jupyter Notebook。它就像是给你的实验台装上了“机械臂”和“透视镜”，效率翻倍。

## 2.1 核心概念：三位一体

在使用 VS Code 写 AI 代码前，你必须理解这三个角色的关系，否则你会经常遇到“找不到模块”的报错。

1. **VS Code:** 你的**操控台**（界面、编辑器）。
2. **Jupyter:** 你的**实验记录本**（文件格式 \`.ipynb\`）。
3. **Anaconda (Conda):** 你的**发动机**（Python 环境）。

**⚠️ 新手第一坑：选择内核 (Select Kernel)**
VS Code 只是一个壳，它需要你告诉它：“用哪个发动机来跑代码？”。

* **操作:** 在 VS Code 打开 \`.ipynb\` 文件后，**右上角**会有一个 \`Select Kernel\` (或者显示 \`Python 3.x.x\`) 的按钮。
* **关键点:** 点击它，必须选择你通过 **Anaconda** 创建的那个环境（例如 \`base\` 或你自定义的 \`ai_env\`）。只有选对了内核，你 \`import pandas\` 才不会报错。

---

## 2.2 VS Code 里的“超能力”

相比于网页版 Jupyter，VS Code 提供了几个极其强大的独家功能，请务必在接下来的练习中尝试：

### 🔍 1. 变量查看器 (Data Viewer) —— 也就是“上帝视角”

在网页版里，你想看数据长什么样，得用 \`print(df)\`。
但在 VS Code 里：

1. 运行产生变量的代码。
2. 点击顶部工具栏的 **"Variables"** 按钮。
3. 你会看到一个列表。点击 DataFrame 旁边的图标，VS Code 会弹出一个**像 Excel 一样的交互式表格**。你可以直接在里面筛选、排序，查看每一行数据，而不需要写一行代码！

### 🧠 2. 智能补全 (IntelliSense)

当你输入 \`pd.re\` 时，VS Code 会自动弹窗提示 \`read_csv\`, \`read_excel\` 等，并显示参数文档。这比在网页里狂按 \`Tab\` 键要流畅得多。

---

## 2.3 实战任务：波士顿房价分析 (VS Code 版)

> **🎯 任务目标:**
> 在 VS Code 中创建一个新的 Notebook，分析房屋面积、房间数与价格的关系，并生成训练数据。

请在 VS Code 中新建一个文件 \`house_price.ipynb\`，按照以下步骤操作。

### 第一步：Pandas —— 数据的“Excel 杀手”

\`\`\`python
# Cell 1
import pandas as pd

# 伪造一份数据
data = {
    '房间数': [1, 2, 3, 4, 5, 2, 6, 8],
    '面积': [30, 50, 80, 100, 120, 55, 150, 200],
    '价格': [100, 160, 250, 320, 380, 170, 480, 650]
}

df = pd.DataFrame(data)

# 运行这一格 (Shift + Enter)
# 👉 交互动作：现在，请点击 VS Code 顶部的 'Variables' 按钮，
# 在弹出的列表中找到 'df'，点击它左侧的小图标。
# 你看到了什么？是不是一个漂亮的表格？
\`\`\`

### 第二步：Matplotlib —— 嵌入式画图

在 VS Code 中，图表会直接渲染在代码单元格下方，并且支持缩放和保存。

\`\`\`python
# Cell 2
import matplotlib.pyplot as plt

# 设置中文字体（避免乱码的常规操作，视系统而定，这里先用英文演示）
plt.figure(figsize=(8, 5))

# 绘制：横轴是面积，纵轴是价格
plt.scatter(x=df['面积'], y=df['价格'], color='#FF5733') # 支持 Hex 颜色代码

plt.title("House Price Analysis")
plt.xlabel("Area (sqm)")
plt.ylabel("Price (10k)")
plt.grid(True, linestyle='--')

plt.show()
\`\`\`

### 第三步：NumPy —— 矩阵化 (关键)

机器学习模型不吃 Excel 表格，只吃矩阵。这一步是将“人类读得懂的数据”转化为“机器读得懂的数据”。

\`\`\`python
# Cell 3
import numpy as np

# .values 是 Pandas 到 NumPy 的桥梁
# X (特征): 我们用 '房间数' 和 '面积' 来预测
X = df[['房间数', '面积']].values 
# y (标签): 我们要预测的目标是 '价格'
y = df['价格'].values

print(f"X Shape: {X.shape}") # 应该是 (8, 2)
print(f"y Shape: {y.shape}") # 应该是 (8,)

# 👉 交互动作：再次查看 Variables 视图
# 观察 X 和 df 的区别。df 是带有表头的，而 X 只是纯粹的数字矩阵。
\`\`\`

---

## 2.4 必须掌握的 VS Code 快捷键

在 VS Code 中写 Notebook，效率来源于键盘：

1. **运行当前格:** \`Shift + Enter\` (最常用)
2. **仅运行当前格不跳转:** \`Ctrl + Alt + Enter\` (调试时很有用)
3. **在上方插入格:** \`A\` (需在命令模式，即光标不在代码框内时)
4. **在下方插入格:** \`B\`
5. **命令面板:** \`F1\` 或 \`Ctrl/Cmd + Shift + P\`。如果在 Jupyter 里迷路了，或者内核连不上，按这个搜 "Reload Window" 通常能解决 90% 的问题。

---

## 2.5 总结：数据流水线

恭喜！你已经掌握了现代 AI 工程师的标准工作流：

1. **环境:** 用 **Anaconda** 管理包。
2. **工具:** 用 **VS Code** 编写和调试代码。
3. **数据:** 用 **Pandas** 读取，用 **Variables View** 检查。
4. **转化:** 用 **NumPy** 将数据转化为矩阵，准备喂给模型。
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'ml-03-numpy',
        title: '数学引擎：NumPy 的矩阵思维与高维魔法',
        description: '从 ndarray 到广播机制，深入理解 AI 的数据基石。',
        content: `
# 第 3 章：数学引擎：NumPy 的矩阵思维与高维魔法

在 Python 的世界里，原生列表（List）虽然灵活，但太慢了。对于深度学习动辄百万级的参数运算，我们需要一辆“F1 赛车”。

**NumPy (Numerical Python)** 就是这辆赛车。它是几乎所有现代 AI 框架（TensorFlow, PyTorch, Scikit-learn）的**底层依赖**。如果你看不懂 NumPy，你甚至无法读懂 AI 模型的报错信息。

本章我们将深入 NumPy 的核心，从最基本的数组结构，一直讲到它是如何支撑神经网络运算的。

---

## 3.1 为什么我们需要 NumPy？(从浅入深)

### 3.1.1 速度的差异

先来看一个直观的例子。假设我们要计算两个包含 100 万个数字的列表相加。

* **Python 列表 (for 循环):** CPU 需要在循环中一次次地取数、判断类型、相加。这叫“单兵作战”。
* **NumPy 数组 (向量化):** 它调用底层的 C 语言代码，利用 CPU 的 SIMD 指令集，一声令下，百万数据同时相加。这叫“集团军作战”。

### 3.1.2 核心数据结构：\`ndarray\`

NumPy 的核心只有一样东西：**N-dimensional Array (N 维数组)**。
它和 Python List 最大的区别在于：

1. **内存连续:** 它在内存里是一块连续的空间（像一排紧挨着的储物柜）。
2. **类型固定:** 里面的元素必须是同一种类型（比如全是整数，或者全是浮点数）。

---

## 3.2 维度 (Dimensions)：AI 工程师的空间想象力

这是初学者最容易晕、也最必须掌握的概念。AI 模型处理的数据，本质上就是不同维度的数组。

请在 VS Code 中运行以下代码，建立空间感：

### 3.2.1 0D 到 3D 的进化

\`\`\`python
import numpy as np

# Level 0: 标量 (Scalar) - 一个点
# 场景：Loss 值，准确率
d0 = np.array(10)
print(f"0D Shape: {d0.shape}, 维度: {d0.ndim}") 

# Level 1: 向量 (Vector) - 一条线
# 场景：音频波形，一个人的特征（身高, 体重, 年龄）
d1 = np.array([1, 2, 3])
print(f"1D Shape: {d1.shape}, 维度: {d1.ndim}") 

# Level 2: 矩阵 (Matrix) - 一个面 (Excel表)
# 场景：灰度图片 (高x宽)，一堆人的特征表
d2 = np.array([
    [1, 2, 3],
    [4, 5, 6]
])
print(f"2D Shape: {d2.shape}, 维度: {d2.ndim}") 
# 输出 (2, 3) -> 代表 2 行 3 列

# Level 3: 张量 (Tensor) - 一个体 (立方体)
# 场景：彩色图片 (高x宽x3通道)，视频流
d3 = np.array([
    [[1, 2], [3, 4]], 
    [[5, 6], [7, 8]]
])
print(f"3D Shape: {d3.shape}, 维度: {d3.ndim}")

\`\`\`

> **🧠 记忆口诀:** 看最左边有几个中括号 \`[\`，就是几维数组。

---

## 3.3 创建数组：不仅仅是手写

在实际 AI 任务中，我们很少手写数字，通常是用“生成器”。

### 3.3.1 特殊数组生成

\`\`\`python
# 1. 全 0 矩阵
# 场景：初始化一个空的画布，或者全黑的图片
zeros = np.zeros((3, 4)) # 生成 3行4列 的全0矩阵

# 2. 全 1 矩阵
# 场景：初始化掩码 (Mask)
ones = np.ones((2, 2))

# 3. 序列生成 (arange)
# 替代 Python 的 range()
sequence = np.arange(0, 10, 2) # [0, 2, 4, 6, 8]

\`\`\`

### 3.3.2 随机数生成 (AI 的起点)

这一点至关重要。**神经网络的权重 (Weights) 在训练开始前，必须随机初始化。**

\`\`\`python
# 1. 标准正态分布 (Standard Normal Distribution)
# 生成均值为0，方差为1的随机数
# 场景：最常用的参数初始化方法
weights = np.random.randn(3, 3) 

# 2. 随机整数
# 场景：从数据集中随机抽取样本（抽奖）
indices = np.random.randint(0, 100, size=5)

\`\`\`

---

## 3.4 变形记：Reshape 的艺术

数据往往不是你要的样子。比如你读取了一张图片，它是一个 \`28x28\` 的矩阵，但你的模型输入层需要一个 \`784\` 长度的向量。

这就需要用到 \`reshape\`。

\`\`\`python
# 创建一个包含 12 个元素的数组
arr = np.arange(12) # [0, 1, ... 11]

# 动作 1: 变成 3行4列
mat = arr.reshape(3, 4)

# 动作 2: 展平 (Flatten)
# 把多维数组拍扁成一维，这在卷积神经网络 (CNN) 到 全连接层 的过渡中必用
flat = mat.reshape(-1)

# 🔥 核心技巧: -1 (自动推导)
# 场景：我有 12 个数据，我想分成 2 列，但我懒得算有几行
auto_mat = arr.reshape(-1, 2) 
# NumPy 会自动算出：12 / 2 = 6 行，所以结果是 (6, 2)

\`\`\`

---

## 3.5 索引与切片：精准手术

想要修改或提取数据，必须精通切片。NumPy 的切片比 Python List 强在它支持**多维切片**。

\`\`\`python
data = np.array([
    [10, 20, 30],
    [40, 50, 60],
    [70, 80, 90]
])

# 任务 1: 取出中间的元素 (50)
print(data[1, 1]) # [行, 列] -> 这里的逗号是 NumPy 独有的语法

# 任务 2: 取出前两行的后两列
# [行切片, 列切片]
# 0:2 代表行取索引 0 和 1
# 1:  代表列取索引 1 到最后
subset = data[0:2, 1:] 
# 结果: [[20, 30], [50, 60]]

# 任务 3: 布尔索引 (Boolean Indexing) - 极其常用！
# 场景：把图片中像素值大于 50 的点（亮斑）过滤出来
mask = data > 50
print(mask) # 这是一个全是 True/False 的矩阵
print(data[mask]) # 直接取出所有 >50 的数字: [60, 70, 80, 90]

\`\`\`

---

## 3.6 核心魔法：广播 (Broadcasting)

这是 NumPy 最“智能”的地方，也是新手最容易出错的地方。

**规则:** 如果两个数组形状不同，NumPy 会尝试自动把小的数组“复制”成大数组的形状，以便进行运算。

### 例子：给全班同学加分

\`\`\`python
scores = np.array([
    [80, 90, 85], # 学生 A 的 语数英
    [70, 75, 80]  # 学生 B 的 语数英
]) # Shape: (2, 3)

bonus = np.array([10, 5, 0]) # Shape: (3,) -> 语文加10分，数学加5分，英语不加

# 直接相加！
final_scores = scores + bonus

# 发生了什么？
# NumPy 发现 bonus 只有 1 行，但 scores 有 2 行。
# 它自动把 bonus 复制了一份，变成了:
# [[10, 5, 0],
#  [10, 5, 0]]
# 然后和 scores 对应位置相加。

\`\`\`

> **⚠️ 避坑:** 广播要求尾部维度必须对齐。比如 (2, 3) 和 (3,) 可以运算，但 (2, 3) 和 (2,) 就会报错！

---

## 3.7 矩阵运算：点积 (Dot Product)

这是深度学习的灵魂。神经网络的前向传播，本质上就是 。

* \`*\` : 对应位置相乘 (Element-wise)。
* \`@\` 或 \`np.dot\` : **矩阵乘法 (Matrix Multiplication)**。

\`\`\`python
a = np.array([[1, 2], [3, 4]])
b = np.array([[1, 0], [0, 1]])

# 对应位置相乘
print(a * b) 
# [[1, 0], [0, 4]]

# 矩阵乘法 (线性代数法则)
# 行乘列，求和
print(a @ b) 
# [[1, 2], [3, 4]]

\`\`\`

---

## 本章作业

打开你的 VS Code，完成以下挑战：

1. 创建一个  的随机矩阵（数值在 0-1 之间）。
2. 将所有大于 0.5 的数值替换为 1，小于等于 0.5 的替换为 0（**提示：** 使用布尔索引）。
3. 这其实就是神经网络中 **Dropout** 或 **激活函数** 的雏形！
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'ml-04-pandas',
        title: '数据工坊：Pandas 的表格艺术',
        description: '从 Series 到 DataFrame，掌握数据清洗、筛选与透视的核心技能。',
        content: `
# 第 4 章：数据工坊：Pandas 的表格艺术

如果说 NumPy 是数学家的算盘，那么 **Pandas** 就是数据分析师的瑞士军刀。

在现实世界中，我们遇到的数据很少是完美的纯数字矩阵。它们往往是杂乱的 Excel 表格、CSV 文件，包含了时间、文本、缺失值等各种混合类型。Pandas 专为处理这种**结构化数据 (Tabular Data)** 而生。

本章我们将学习如何像外科医生一样精准地操作表格数据。

## 4.1 核心数据结构：Series 与 DataFrame

Pandas 的世界由两个主角构成：

### 1. Series (一维)
它就像 Excel 表格中的**一列**。
*   **组成:** 数据值 (Values) + 索引 (Index)。
*   **特点:** 索引不一定是数字，可以是任何标签（比如股票代码、日期）。

### 2. DataFrame (二维)
它就是**整个 Excel 表格**。
*   **组成:** 多个 Series 拼在一起。
*   **特点:** 有行索引 (Index) 和列名 (Columns)。

## 4.2 数据的“CRUD”

*   **Create (创建/读取):** \`pd.read_csv()\`, \`pd.read_excel()\`
*   **Read (查看):** \`.head()\` (看前几行), \`.info()\` (看类型和内存), \`.describe()\` (看统计摘要)
*   **Update (修改):** 赋值操作
*   **Delete (删除):** \`.drop()\`

## 4.3 数据的“手术刀”：索引与切片

Pandas 提供了两种强大的索引方式，这是新手最容易混淆的地方：

*   **\`loc\` (Label-based):** 看标签。
    *   \`df.loc['2023-01-01', '温度']\` -> 找索引为 '2023-01-01' 那行的 '温度' 列。
*   **\`iloc\` (Integer-based):** 看位置 (0, 1, 2...)。
    *   \`df.iloc[0, 1]\` -> 找第 0 行，第 1 列。

> **⚠️ 避坑:** \`loc\` 切片 **包含** 末尾 (Inclusive)，而 \`iloc\` **不包含** 末尾 (Exclusive，像 Python 列表一样)。

## 4.4 数据的“维修站”：处理缺失值

真实数据往往是“脏”的，充满了 \`NaN\` (Not a Number)。
模型遇到 \`NaN\` 会直接报错。我们通常有两种策略：

1.  **直接丢弃:** \`df.dropna()\` -> 只要有空值就整行删除（适合数据量大且缺失少的情况）。
2.  **填充修补:** \`df.fillna()\` -> 用平均值、中位数或 0 去填补空缺。

## 4.5 数据的“透视表”：GroupBy

这是 Pandas 最强大的功能之一，对应 SQL 中的 \`GROUP BY\`。
它遵循 **Split-Apply-Combine** (拆分-应用-合并) 的逻辑：

1.  **Split:** 按某个标准（比如“班级”）把数据拆开。
2.  **Apply:** 对每组数据应用函数（比如求“平均分”）。
3.  **Combine:** 把结果合并成一张新表。

\`\`\`python
# 算出每个城市的平均温度
df.groupby('城市')['温度'].mean()
\`\`\`
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'ml-05-visualization',
        title: '透视数据真相：Matplotlib 与 Seaborn 深度实战',
        description: '从底层画师到高级装修，掌握数据可视化的核心套路。',
        content: `
# 第 5 章：透视数据真相：Matplotlib 与 Seaborn 深度实战

在数据科学界，**可视化 (Visualization)** 是我们与数据对话的语言。
如果只看枯燥的表格（DataFrame），你可能会错过很多关键信息：

* **趋势 (Trend):** 股票是涨是跌？
* **分布 (Distribution):** 用户年龄是集中在 20 岁还是 50 岁？
* **离群值 (Outliers):** 有没有数据是错误的，或者有异常行为？
* **关系 (Correlation):** 广告费投得越多，销量真的越高吗？

本章我们将结合 **Matplotlib (底层引擎)** 和 **Seaborn (高级装修)**，带你由浅入深掌握这项技能。

---

## 5.1 核心原理：为什么需要两个库？

很多初学者会困惑：*“既然 Seaborn 画图那么漂亮，为什么还要学 Matplotlib？”*

要回答这个问题，我们需要理解它们的关系：

### 1. Matplotlib：底层的“画师”

* **定位:** Python 绘图的**基石**。
* **原理:** 它模仿了 MATLAB 的绘图接口，提供了对图形最细粒度的控制。它能控制画布上的每一个像素、每一条线的粗细、每一个刻度的字体。
* **缺点:** 代码繁琐（画一个好看的统计图可能需要 20 行代码），默认配色比较“复古”（不够现代）。

### 2. Seaborn：高级的“装修队”

* **定位:** 基于 Matplotlib 封装的**统计绘图库**。
* **原理:** 它在 Matplotlib 之上穿了一层“漂亮的衣服”。它集成了 Pandas 的数据结构，内置了统计功能（比如自动计算置信区间、自动拟合回归线）。
* **优点:** 极简代码（1 行顶 Matplotlib 10 行），默认配色现代美观。

### ⚖️ 异同对比表

| 特性 | Matplotlib | Seaborn |
| --- | --- | --- |
| **层级** | 底层 (Low-level) | 高级 (High-level) |
| **数据支持** | 主要处理 NumPy 数组 | 完美支持 Pandas DataFrame |
| **灵活性** | ⭐⭐⭐⭐⭐ (无所不能) | ⭐⭐⭐ (固定模板) |
| **统计功能** | 无 (需要自己算好再画) | 强 (自动聚合、拟合) |
| **使用场景** | **调整细节** (改标题、轴标签、布局) | **快速分析** (看分布、看相关性) |

---

## 5.2 绘图的“万能公式” (General Workflow)

这是本章**最重要**的知识点。很多新手代码写得乱，是因为混用了两种写法。
在工业界，标准的**面向对象 (Object-Oriented)** 绘图套路如下：

### 第一步：搭台子 (Create Canvas)

使用 Matplotlib 创建**画布 (Figure)** 和 **子图/坐标系 (Axes)**。

\`\`\`python
# fig 是整张画纸，ax 是画纸上具体的“一个格子”
fig, ax = plt.subplots(figsize=(8, 6))
\`\`\`

### 第二步：画内容 (Plotting)

使用 Seaborn 将图画在指定的 \`ax\` 上。

\`\`\`python
# 注意 ax=ax 参数，意思是“把图画在这个格子里”
sns.scatterplot(data=df, x='...', y='...', ax=ax)
\`\`\`

### 第三步：修细节 (Customization)

使用 Matplotlib 的 \`ax\` 方法修改标题、标签等。

\`\`\`python
ax.set_title("我的标题")
ax.set_xlabel("X轴名称")
\`\`\`

### 第四步：展示 (Show/Save)

\`\`\`python
plt.show()
# 或 plt.savefig('output.png')
\`\`\`

---

## 5.3 基础招式：四大金刚

在掌握了复杂的统计图之前，我们需要先用好最基础的四种图表。它们能覆盖 80% 的日常汇报需求。

### 1. 折线图 (Line Chart)
*   **用途:** 展示**趋势** (随时间变化)。
*   **代码:** \`sns.lineplot(x='date', y='value', data=df)\`

### 2. 散点图 (Scatter Plot)
*   **用途:** 展示两个变量之间的**关系** (相关性)。
*   **代码:** \`sns.scatterplot(x='height', y='weight', data=df)\`

### 3. 柱状图 (Bar Chart)
*   **用途:** 展示不同类别的**对比**。
*   **代码:** \`sns.barplot(x='category', y='value', data=df)\`

### 4. 饼图 (Pie Chart)
*   **用途:** 展示**占比** (整体与部分)。
*   **注意:** Seaborn 不支持饼图（因为它被认为是不精确的），我们需要用 Matplotlib 画。
*   **代码:** \`plt.pie(x, labels=labels)\`

---

## 5.4 进阶招式：统计洞察 (Statistical Insights)

基础图表只能看表面，统计图表能帮我们挖掘数据内部的分布规律。

### 1. 分布分析：直方图 (Histogram)
**问题:** 客人们通常消费多少钱？是集中在低消费，还是贫富差距很大？

\`\`\`python
fig, ax = plt.subplots(figsize=(8, 5))
# kde=True 显示核密度曲线
sns.histplot(data=df, x='total_bill', kde=True, bins=20, color='teal', ax=ax)
ax.set_title('顾客消费金额分布')
plt.show()
\`\`\`

### 2. 类别对比：箱线图 (Boxplot)
**问题:** 周末大家会比平时更大方吗？有哪些异常的“土豪”？

\`\`\`python
fig, ax = plt.subplots(figsize=(8, 6))
# 箱线图能清晰地展示中位数、四分位数和离群点
sns.boxplot(data=df, x='day', y='total_bill', palette='Set2', ax=ax)
ax.set_title('不同日期的消费分布 (看离群点)')
plt.show()
\`\`\`

---

## 5.5 终极奥义：多变量热力图 (Heatmap)

**问题:** 所有的数字特征中，哪两个关系最紧密？

这是特征工程中**最重要的图**，用于筛选特征。

\`\`\`python
fig, ax = plt.subplots(figsize=(8, 6))

# 1. 计算相关系数矩阵 (-1 到 1)
# numeric_only=True: 只计算数字列
corr = df.corr(numeric_only=True)

# 2. 绘制热力图
sns.heatmap(
    corr, 
    annot=True,        # 在格子里显示数字
    cmap='coolwarm',   # 颜色：红=正相关，蓝=负相关
    fmt=".2f",         # 保留两位小数
    linewidths=0.5,    # 格子间距
    ax=ax
)

ax.set_title('特征相关性热力图')
plt.show()
\`\`\`

---

## 本章总结

通过本章，我们不仅学会了代码，更掌握了**“绘图思维”**：

1. **先搭台子:** 永远用 \`fig, ax = plt.subplots()\` 起手。
2. **选对工具:**
   * 看趋势 -> **Lineplot**
   * 看关系 -> **Scatterplot**
   * 看对比 -> **Barplot**
   * 看分布 -> **Histplot**
   * 抓异常 -> **Boxplot**
   * 选特征 -> **Heatmap**
3. **修整细节:** 用 \`ax.set_title\` 等方法让图表专业化。

**下一章预告：**
现在的你，既能清洗数据（Pandas），又能洞察数据（Seaborn）。
是时候让 AI 登场了！在 **第 6 章**，我们将使用 **Scikit-Learn**，把这些经过清洗和洞察的数据喂给算法，训练你的第一个**房价预测模型**。
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'ml-06-sklearn',
        title: '你好，机器学习：Scikit-Learn 极简实战',
        description: 'Scikit-Learn 是 AI 工程师的瑞士军刀。掌握“实例化-训练-预测”三板斧，跑通你的第一个回归模型。',
        content: `
# 第 6 章：你好，机器学习：Scikit-Learn 极简实战

**Scikit-Learn (sklearn)** 是 AI 工程师的瑞士军刀。

* 它不适合做深层神经网络（那是 PyTorch 的地盘）。
* 但除此之外的**一切**（回归、分类、聚类、降维），它都是工业界的首选标准。

它的伟大之处在于**“统一”**。无论你是在做简单的线性回归，还是复杂的随机森林，代码的写法几乎是一模一样的。

---

## 6.1 核心心法：Scikit-Learn 的“三板斧”

在写代码之前，请把这张流程图刻在脑子里。Scikit-Learn 中 99% 的算法都遵循这个套路：

1. **实例化 (Instantiate):** 选择一个模型。
* 代码: \`model = LinearRegression()\`
* *比喻: 去人才市场雇佣了一个什么都没学的“新员工”。*


2. **训练 (Fit):** 喂给它数据，让它找规律。
* 代码: \`model.fit(X, y)\`
* *比喻: 让员工去“上岗培训”，给他看历史考题 (X) 和答案 (y)。*


3. **预测 (Predict):** 用它来处理新数据。
* 代码: \`model.predict(X_new)\`
* *比喻: 员工正式干活，处理新来的业务。*



---

## 6.2 实战案例：预测房价 (Linear Regression)

> **🎯 任务目标:**
> 既然我们是入门，就从最经典的“线性回归”开始。
> 我们有一组房屋数据（面积 vs 价格），我们要训练一个 AI，让它告诉我：**一套 130 平米的房子，大概能卖多少钱？**

### 第一步：准备“干净”的数据

我们先用 Pandas 和 NumPy 模拟一份数据。

\`\`\`python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 1. 模拟数据 (在真实项目中，这里是 pd.read_csv)
# 为了演示，我们生成 50 个样本
np.random.seed(42) # 固定随机种子，保证你跑出来的结果和我一样
areas = np.linspace(50, 200, 50) # 面积: 50平米 到 200平米

# 价格 = 面积*3.5 + 底价10万 + 噪声(40万以内的波动)
prices = areas * 3.5 + 10 + np.random.normal(0, 40, 50) 

# 2. 封装成 DataFrame
df = pd.DataFrame({
    '面积': areas,
    '价格': prices
})

# 3. 看一眼数据
plt.figure(figsize=(8, 5))
sns.scatterplot(data=df, x='面积', y='价格')
plt.title("房价分布图")
plt.show()

\`\`\`

### 第二步：特征 (X) 与 标签 (y) 的分离

这是初学者最容易报错的地方。Scikit-Learn 对数据的形状有严格的**数学约定**：

* **X (特征/题目):** 必须是 **二维矩阵 (2D Matrix)**。哪怕只有一个特征（面积），它也得是 \`50行 x 1列\` 的矩阵。**变量名通常大写**。
* **y (标签/答案):** 通常是 **一维向量 (1D Vector)**。**变量名通常小写**。

\`\`\`python
# 准备 X (特征)
# .values 把 DataFrame 变成 NumPy 数组
# .reshape(-1, 1) 把它强行变成 (50, 1) 的二维矩阵！如果不加这一步会报错！
X = df['面积'].values.reshape(-1, 1) 

# 准备 y (标签)
y = df['价格'].values

print(f"X shape: {X.shape}") # 必须是 (50, 1)
print(f"y shape: {y.shape}") # 必须是 (50,)

\`\`\`

### 第三步：最重要的概念 —— 训练集与测试集拆分

**为什么不能把所有数据都拿来训练？**
这就好比老师考试。如果你把期末考试的**原题和答案**都给学生背（全量训练），学生可能考 100 分，但他并没有学会解题，只是记住了答案。这叫**过拟合 (Overfitting)**。

我们需要把数据切成两块：

* **训练集 (Training Set):** 80% 的数据。用来做课后作业，让模型学习。
* **测试集 (Test Set):** 20% 的数据。用来做期末考试，**模型在训练时绝对不能看**。

\`\`\`python
from sklearn.model_selection import train_test_split

# 自动拆分数据
# test_size=0.2 代表 20% 做测试集
# random_state=42 保证每次切分的结果一样
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"训练集数量: {len(X_train)} | 测试集数量: {len(X_test)}")

\`\`\`

### 第四步：三行代码训练模型

见证奇迹的时刻到了。虽然线性回归背后的数学原理（最小二乘法）涉及矩阵求导，但用 Scikit-Learn 只需要三行。

\`\`\`python
from sklearn.linear_model import LinearRegression

# 1. 实例化 (雇佣员工)
model = LinearRegression()

# 2. 训练 (上岗培训)
# ⚠️ 注意：我们只把“训练集”喂给它！千万别让它偷看测试集！
model.fit(X_train, y_train)

# 3. 看看它学到了什么
# 线性回归公式: y = w * x + b
print(f"学到的权重 (w): {model.coef_[0]:.2f}") 
print(f"学到的截距 (b): {model.intercept_:.2f}")

# 真实的 w 是 3.5，看看模型算出来的接近吗？

\`\`\`

### 第五步：模型评估与可视化

模型训练好了，它到底准不准？我们要让它在“测试集”上做一次期末考试。

\`\`\`python
from sklearn.metrics import mean_absolute_error, mean_squared_error

# 1. 预测 (让模型做测试卷)
# 我们给它 X_test，让它猜 y_pred
y_pred = model.predict(X_test)

# 2. 评估 (老师改卷子)
# MAE: 平均绝对误差 (预测值和真实值平均差了多少钱)
mae = mean_absolute_error(y_test, y_pred)
print(f"平均误差 (MAE): {mae:.2f} 万")

# 3. 可视化：画出那条回归线
plt.figure(figsize=(8, 5))
# 画散点 (真实数据)
plt.scatter(X_test, y_test, color='blue', label='Real Price')
# 画线 (预测结果)
plt.plot(X_test, y_pred, color='red', linewidth=3, label='Prediction Line')

plt.title("线性回归预测结果")
plt.xlabel("面积")
plt.ylabel("价格")
plt.legend()
plt.show()

\`\`\`

**🧐 怎么看这张图？**

* 红线是模型认为的“房价规律”。
* 蓝点是真实的房价。
* 红线离蓝点越近，说明模型越准。

---

## 6.3 举一反三：如果我想换个模型？

假如我觉得线性回归太简单了，我想用高大上的**“决策树 (Decision Tree)”**或者**“随机森林 (Random Forest)”**，代码需要重写吗？

**完全不需要！** 这就是 sklearn 的魅力。你只需要改动**第一行**（实例化）的代码：

\`\`\`python
# 只要改这里！
# from sklearn.linear_model import LinearRegression
# model = LinearRegression()

# 变成决策树回归：
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor() # 换个员工

# 下面的代码一字不用改！
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

\`\`\`

---

## 本章小结

恭喜你！你已经跑通了机器学习的**全流程 (Pipeline)**：

1. **准备数据:** 搞定 X (矩阵) 和 y (向量)。
2. **拆分:** \`train_test_split\` 切分训练集和测试集。
3. **训练:** \`fit()\`。
4. **预测:** \`predict()\`。

**下一章预告：**
现在的模型只能处理“面积”这一个数字。但如果有“房屋朝向”（东南西北）这种**文字特征**怎么办？模型不认识汉字啊。
在 **第 7 章**，我们将学习 **特征工程 (Feature Engineering)**，学习如何把文本、类别标签转化为数字，也就是大名鼎鼎的 **One-Hot 编码**。
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'ml-07-feature-engineering',
        title: '特征工程：让数据“开口说话”',
        description: '数据和特征决定了机器学习的上限。学会类别编码、特征缩放与构造，把原始数据改造成模型最爱吃的样子。',
        content: `
# 第 7 章：特征工程：让数据“开口说话”

业界流传着一句至理名言：

> **“数据和特征决定了机器学习的上限，而模型和算法只是在逼近这个上限。”**

在前面的章节中，我们的数据往往是现成的数字。但在现实世界中，数据是杂乱无章的：有文本（“北京”、“上海”）、有量纲差异巨大的数字（身高 1.8米 vs 工资 20000元）、甚至有非线性的关系。

本章我们将学习特征工程的 **“三大战役”**，把原始数据改造成模型最爱吃的样子。

---

## 7.1 第一战：类别编码 (Categorical Encoding) —— 翻译官

AI 模型本质上是数学公式，它只认识数字（0, 1, 2...），不认识汉字或单词。我们需要把文字“翻译”成数字。

### 7.1.1 数据的两种“身份”

在翻译前，必须先判断数据的性质，否则会酿成大错：

1. **有序类别 (Ordinal):** 有等级、大小之分。
* *例子:* 尺码 (S < M < L)、学历 (本科 < 硕士 < 博士)。
* *对策:* **标签编码 (Label Encoding)**。


2. **无序类别 (Nominal):** 平等关系，无大小之分。
* *例子:* 颜色 (红/蓝/绿)、城市 (北京/上海)、性别 (男/女)。
* *对策:* **独热编码 (One-Hot Encoding)**。



### 7.1.2 战术 A：标签编码 (Label Encoding)

对于有序数据，我们可以直接将其映射为整数：。

\`\`\`python
import pandas as pd

# 1. 模拟衣服尺码数据
df = pd.DataFrame({'尺码': ['S', 'XL', 'M', 'L', 'S', 'M']})

# 2. 自定义映射字典 (必须人为指定顺序，否则机器不知道 S 和 L 谁大)
size_mapping = {
    'S': 1,
    'M': 2,
    'L': 3,
    'XL': 4
}

# 3. 翻译
df['尺码_Code'] = df['尺码'].map(size_mapping)
print(df)

\`\`\`

*结果:* S 变成了 1，XL 变成了 4。模型能理解 ，逻辑正确。

### 7.1.3 战术 B：独热编码 (One-Hot Encoding)

对于“城市”这种无序数据，如果我们强行标记为 \`北京=1, 上海=2\`，模型会错误地认为 \`上海 > 北京\`。
**独热编码**的逻辑是：有多少种分类，就新建多少个“开关”列。

\`\`\`python
# 1. 模拟城市数据
df_city = pd.DataFrame({'城市': ['北京', '上海', '深圳', '北京']})

# 2. 独热编码 (Pandas 神器 get_dummies)
# prefix: 新列名的前缀
df_onehot = pd.get_dummies(df_city, columns=['城市'], prefix='City')

# 3. 转换为整数 (True/False -> 1/0)
df_onehot = df_onehot.astype(int)
print(df_onehot)

\`\`\`

*结果:*
数据变成了三列：\`City_北京\`, \`City_上海\`, \`City_深圳\`。如果是北京，则 \`City_北京\` 为 1，其余为 0。

> **⚠️ 进阶警示 (维度爆炸):** 如果你的类别特别多（如 1 万个商品 ID），使用 One-Hot 会产生 1 万列特征，导致计算崩溃。此时需要使用更高级的 Target Encoding 或 Embedding（深度学习）。

---

## 7.2 第二战：特征缩放 (Feature Scaling) —— 统一度量衡

这是新手最容易忽略的步骤，也是**导致模型准确率低**的头号杀手。

### 7.2.1 为什么要缩放？

假设我们要预测房屋价值，有两个特征：

* **面积:** 50 ~ 200 (平米)
* **房间数:** 1 ~ 5 (个)

在 KNN（计算距离）或 神经网络（梯度下降）眼中，**面积的变化幅度（几百）远大于房间数（个位数）**。
**后果:** 模型会认为“面积”重要一万倍，完全忽略“房间数”的存在。

### 7.2.2 战术 A：标准化 (Standardization / Z-Score) **(⭐强烈推荐)**

将数据转换为 **均值为 0，标准差为 1** 的分布。


* **适用:** 绝大多数算法（SVM, 逻辑回归, 线性回归, 神经网络）。
* **特点:** 不容易受异常值影响。

### 7.2.3 战术 B：归一化 (Min-Max Scaling)

将数据强行压缩到 \`[0, 1]\` 区间。


* **适用:** 对范围有严格要求的场景（如图像像素 0-255）。
* **缺点:** 如果有一个异常大的值，其他正常值都会被压缩到 0 附近。

### 💻 代码实战

\`\`\`python
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import numpy as np

# 模拟数据：两个特征量纲差距巨大
X = np.array([
    [50, 1],   # 小房子
    [200, 5],  # 大别墅
    [100, 2]   # 中等房
])

# 1. 实例化标准化器
scaler = StandardScaler()

# 2. 训练并转换
# 注意：fit_transform = 先计算均值方差(fit)，再执行转换(transform)
X_scaled = scaler.fit_transform(X)

print("原始数据:\n", X)
print("\n缩放后 (Z-Score):\n", X_scaled)

\`\`\`

*结果:* 所有数据都变成了 -1 到 1 之间的小数，模型可以公平对待了。

> **🚨 黄金法则:** 在做机器学习项目时，**必须**遵循：
> 1. 在训练集上 \`fit\` (计算均值)。
> 2. 在训练集上 \`transform\`。
> 3. 在测试集上 **只做** \`transform\` (**绝对不能在测试集上 fit！** 否则就是作弊，因为你偷看了测试集的分布)。
> 
> 

---

## 7.3 第三战：特征变换与构造 —— 挖掘机

有时候，原始数据不够好，我们需要手动“捏”出新特征。

### 7.3.1 数据分箱 (Binning): 处理非线性

有些特征和结果不是线性的。比如“年龄”与“健康风险”通常是 **U 型曲线**（幼年和老年风险高，青壮年风险低）。
线性模型画不出 U 型线，我们可以把年龄切成段（少年、青年、老年），变成类别特征。

\`\`\`python
# Pandas cut 分箱
df['年龄段'] = pd.cut(df['年龄'], bins=[0, 18, 50, 100], labels=['少年', '青年', '老年'])
# 然后再做 One-Hot 编码

\`\`\`

### 7.3.2 特征构造 (Feature Interaction): 创造新知识

模型是很笨的。如果你给它“长”和“宽”，它不一定知道“面积”就是长乘以宽。我们需要帮它造出来。

* **多项式特征 (Polynomial Features):** 自动生成  等特征。

\`\`\`python
from sklearn.preprocessing import PolynomialFeatures

# 原始特征: [长, 宽]
X = [[2, 3], [4, 5]]

# 构造 2 次多项式
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# 结果特征变成了 5 个: [长, 宽, 长^2, 长*宽, 宽^2]
# 其中 "长*宽" 就是面积！这一步极大提升了模型的表现。

\`\`\`

---

## 本章总结：特征工程检查清单

在把数据喂给模型之前，请按顺序检查以下事项：

1. **翻译 (Encoding):**
* 有序文字  Label Encoding
* 无序文字  One-Hot Encoding


2. **度量 (Scaling):**
* 所有数字特征  StandardScaler (标准化)


3. **挖掘 (Construction):**
* 非线性关系  分箱 (Binning)
* 隐式关系  多项式特征/业务组合特征
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'ml-08-algorithm-map',
        title: '算法地图：如何为你的问题选择“最强武器”',
        description: '回归、分类还是聚类？通过一张地图理清机器学习的脉络，告别选择困难症。',
        content: `
# 第 8 章：算法地图：如何为你的问题选择“最强武器”

面对一个 AI 问题，新手往往上来就问：“代码怎么写？”
而高手会先问三个问题：

1. **你的数据有标签（答案）吗？**
2. **你要预测的是一个数字，还是一个类别？**
3. **你的数据量有多大？**

本章我们将通过**“任务类型”**和**“算法家族”**两个维度，帮你彻底理清机器学习的脉络。

---

## 8.1 维度一：三大任务 (你想要什么？)

根据**“有没有标准答案（标签 y）”**以及**“预测目标的类型”**，我们将机器学习分为三大核心任务。

### 1. 回归 (Regression) —— 预测“多少”

* **属性:** **有监督学习** (有题有答案)。
* **目标:** 预测一个**连续的数值**。
* **直观理解:** 画一条线（拟合），尽可能穿过所有的数据点。

> **🌰 生动案例：奶茶店长的烦恼**
> 你是奶茶店店长。你想知道**“明天到底要煮多少斤珍珠？”**（预测目标是连续的数字：可能是 5.5 斤，也可能是 8.2 斤）。
> * 你参考了过去 30 天的气温、是不是周末、有没有打折活动。
> * 这就是典型的**回归问题**。

### 2. 分类 (Classification) —— 预测“是谁”

* **属性:** **有监督学习** (有题有答案)。
* **目标:** 预测一个**离散的类别**。
* **直观理解:** 画一条边界（Decision Boundary），把红点和蓝点分开。

> **🌰 生动案例：急诊室的分诊台**
> 你是急诊科医生。病人送来时，你需要立刻判断：**“是普通感冒（回家吃药），还是急性阑尾炎（马上手术）？”**
> * 结果只有两个选项：A 或 B（或者更多选项 C）。不存在“半个阑尾炎”。
> * 这就是典型的**分类问题**。

### 3. 聚类 (Clustering) —— 寻找“圈子”

* **属性:** **无监督学习** (有题没答案)。
* **目标:** 没有标签 ，只有特征 。让 AI 自己发现数据内部的**结构**和**相似性**。
* **直观理解:** 物以类聚，人以群分。

> **🌰 生动案例：哈利波特的分院帽**
> 新生入学了，分院帽并不知道每个学生叫什么，也没人告诉它谁属于哪个学院。
> * 它只能根据学生的特征（勇敢、精明、勤劳）把性格相似的学生**堆**在一起。
> * 最后它说：“这一堆是格兰芬多，那一堆是斯莱特林。”
> * 这就是典型的**聚类问题**。

---

## 8.2 维度二：武器库盘点 (你用什么打？)

Scikit-Learn 中的算法虽然多，但其实主要就分这几大家族。

### 🏰 第一家族：线性模型 (The Linear Family)

* **代表算法:** 线性回归 (回归), 逻辑回归 (分类)。
* **原理:** 试图画一条**直线**（或平面）来拟合或分割数据。
* **优点:** 速度极快，可解释性最强。

> **🌰 生动案例：切西瓜**
> 逻辑回归就像是用一把**直尺**去切西瓜。它假设好瓜和坏瓜之间有一条笔直的分界线。虽然简单粗暴，但对于大部分明显的坏瓜，一刀切下去非常有效。

### 🌲 第二家族：树模型 (The Tree Family) —— **表格数据之王**

* **代表算法:** 决策树, 随机森林, XGBoost。
* **原理:** 像很多个“如果-那么” (If-Else) 的规则组合。
* **优点:** 准确率高，能处理非线性关系（不需要画直线）。

> **🌰 生动案例：相亲问卷 / 猜人游戏**
> 决策树就像是在玩“20个问题”游戏：
> * 问题1：他是男的吗？（是/否）
> * 问题2：他有房吗？（是/否）
> * 问题3：他身高超过180吗？（是/否）
> * **结论：** 这是一个优质对象。
> * **随机森林**就是找了 100 个媒婆同时帮你问，最后大家投票决定。

### 📐 第三家族：支持向量机 (SVM)

* **代表算法:** SVC (分类), SVR (回归)。
* **原理:** 试图找到一条最宽的“马路”（最大间隔）来分隔不同类别。
* **适用场景:** 数据量不大，但维度特别高。

> **🌰 生动案例：两国划界**
> 两个国家打仗（两类数据）。SVM 不仅仅是想画一条国界线，它是想画一条**最宽的非军事区（DMZ）**。它希望这条河越宽越好，这样两边的士兵（数据点）就离得越远，不容易产生摩擦（分类错误）。

### 🤝 第四家族：近邻算法 (KNN)

* **代表算法:** K-Nearest Neighbors.
* **原理:** “近朱者赤”。新数据像谁，它就是谁。

> **🌰 生动案例：孟母三迁**
> 这里的房价值多少钱？KNN 不算公式，它直接看：
> * 左边邻居卖 500 万。
> * 右边邻居卖 510 万。
> * 对门邻居卖 490 万。
> * **结论：** 那你这房子大概率也值 500 万左右。

---

## 8.3 决策向导：如何选型？

Scikit-Learn 官方提供了一张著名的**算法选择路径图**。我们将这张图简化为以下的决策逻辑：

### Step 1: 你的数据量够吗？

* **< 50 个样本:** 别做机器学习了。
* *例子:* 你只调查了全班 30 个人的身高，想预测全人类的身高？这叫统计学上的“不可信”。

### Step 2: 你有标签 (y) 吗？

* **没有 (无监督):**
  * 要做分群 -> **K-Means 聚类**。

* **有 (有监督):** 进入 Step 3。

### Step 3: 你的 y 是什么类型？

* **y 是连续数字 (如房价):** -> **回归任务**
  * 先试 **Lasso / 线性回归** (简单快速)。
  * 精度不够？上 **XGBoost / 随机森林** (重型武器)。

* **y 是离散类别 (如是否违约):** -> **分类任务**
  * 先试 **逻辑回归** (基准线)。
  * 精度不够？上 **XGBoost / 随机森林**。

---

## 8.4 避坑指南：没有免费的午餐

AI 界有一个著名的 **“没有免费午餐定理” (No Free Lunch Theorem)**：

> **没有一个算法在所有问题上都是最强的。**

> **🌰 生动案例：瑞士军刀 vs 手术刀**
> * **逻辑回归** 就像 **瑞士军刀**：轻便、好用、随身携带，能解决 80% 的日常问题（切水果、拧螺丝）。
> * **深度学习 (神经网络)** 就像 **精密手术刀**：它能做开颅手术（图像识别、自然语言处理），但如果你只是想削个苹果（简单的表格预测），用手术刀不仅杀鸡用牛刀，还容易割伤手（过拟合）。

---

## 本章小结

现在的你，手里已经拿到了地图。

| 任务 | 核心目标 | 🌰 **一句话例子** | 首选算法 |
| --- | --- | --- | --- |
| **回归** | 预测数字 | 预测**明天多少度** | 线性回归, XGBoost |
| **分类** | 预测类别 | 预测**明天是否下雨** | **逻辑回归**, 随机森林 |
| **聚类** | 发现分组 | **给学生分班** | K-Means |
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'lr-01',
        title: '线性回归 (Linear Regression)',
        description: '理解最基础的预测模型，学习拟合直线与损失函数。',
        content: `
# 线性回归：预测的艺术

线性回归不仅是机器学习中最基础的算法，更是统计学的基石。它的核心思想非常直观：**在杂乱的数据中找到一种简单的线性关系**。

> "所有的模型都是错误的，但有些是有用的。" —— George Box

## 1. 数学模型

假设我们有一组数据点，我们希望用一条直线去拟合它们。在二维平面上，这条直线的公式为：

$$ y = wx + b $$

其中：
- $w$ (Weight) 是**权重**，决定了直线的斜率。它代表了特征的重要性。
- $b$ (Bias) 是**偏置**，决定了直线的截距。它允许直线上下平移。

> [!NOTE]
> 在高维空间中，这条“直线”被称为**超平面 (Hyperplane)**。

## 2. 损失函数 (MSE)

我们怎么知道哪条线是最好的？我们需要一个标准来衡量“好坏”，这就是**损失函数 (Loss Function)**。

对于回归问题，最常用的是**均方误差 (Mean Squared Error, MSE)**：

$$ L = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred}^{(i)} - y_{true}^{(i)})^2 $$

我们的目标很简单：**找到一组 $w$ 和 $b$，使得 $L$ 最小。**

## 3. 优化：梯度下降 (Gradient Descent)

想象你在漆黑的山顶，想要下山。你看不清路，但你能感觉到脚下的坡度。你会往**最陡峭的下坡方向**迈一步。这就是梯度下降。

$$ w_{new} = w_{old} - \\alpha \\cdot \\frac{\\partial L}{\\partial w} $$

- $\\alpha$ 是**学习率 (Learning Rate)**，决定了你步子迈多大。
- $\\frac{\\partial L}{\\partial w}$ 是**梯度**，指引了下山的方向。

\`\`\`quiz
{
  "question": "如果学习率 (Learning Rate) 设置得太大，会发生什么？",
  "options": [
    "模型训练会非常慢",
    "模型会非常精准",
    "Loss 可能会震荡甚至发散，无法收敛",
    "计算速度会变慢"
  ],
  "answer": 2,
  "explanation": "学习率太大意味着步子迈得太大，可能会直接越过最低点，导致在山谷两侧来回震荡，甚至爬到对面的山上去了（发散）。"
}
\`\`\`

---
**交互练习**：请查看下方的"交互实验室"，尝试调整学习率，观察 Loss 曲线是如何收敛的。
`.trim(),
        visualizerType: 'loss-chart'
      },
      {
        id: 'dt-02',
        title: '决策树 (Decision Trees)',
        description: '像人类思考一样进行分类与决策。',
        content: `
# 决策树：像人类一样思考

决策树 (Decision Tree) 是一种模仿人类决策过程的算法。它将复杂的决策分解为一系列简单的“是/否”问题。

## 1. 结构解析

决策树由三种节点组成：
*   **根节点 (Root Node)**: 包含所有样本，是决策的起点。
*   **内部节点 (Internal Node)**: 表示在一个特征上的测试（例如：“天气是晴天吗？”）。
*   **叶节点 (Leaf Node)**: 代表最终的决策结果或类别。

## 2. 如何分裂？(Splitting)

构建决策树的关键在于：**在每个步骤，选择哪个特征进行分裂？**
我们的目标是让分裂后的子集尽可能“纯净”。

### 关键概念：熵 (Entropy)
熵是衡量系统混乱程度的指标。
- 假如一个盒子里全是红球，**熵低**（有序）。
- 假如盒子里红球蓝球各一半，**熵高**（混乱）。

$$ H(S) = - \\sum p_i \\log_2(p_i) $$

### 信息增益 (Information Gain)
我们选择那个能让**熵下降最多**（即信息增益最大）的特征进行分裂。

$$ IG(S, A) = H(S) - \\sum \\frac{|S_v|}{|S|} H(S_v) $$

## 3. 剪枝 (Pruning)

决策树很容易**过拟合 (Overfitting)**。如果树长得太深，它可能会记住了训练数据中的噪声。
为了防止这种情况，我们需要“剪枝”——去掉那些对预测贡献不大的分支。
`.trim(),
        visualizerType: 'none'
      }
    ]
  },
  {
    id: 'dl-101',
    title: '深度学习入门',
    description: '深入神经网络，学习反向传播与多层感知机。',
    icon: 'Network',
    color: 'from-violet-500 to-purple-600',
    difficulty: Difficulty.Intermediate,
    lessons: [
      {
        id: 'nn-01',
        title: '神经网络架构',
        description: '神经元、层与激活函数的工作原理。',
        content: `
# 神经网络：大脑的数学模拟

人工神经网络 (Artificial Neural Networks, ANN) 是深度学习的核心。它受生物神经元启发，虽然简化了许多生物细节，但却捕捉到了并行处理和学习的精髓。

## 1. 感知机 (Perceptron)

最简单的神经网络单元。它接收输入，加权求和，然后通过激活函数。

$$ output = f(\\sum w_i x_i + b) $$

## 2. 网络结构

现代神经网络通常包含三个部分：

1.  **输入层 (Input Layer)**: 接收原始数据（像素、文本向量等）。
2.  **隐藏层 (Hidden Layers)**: 这是“深度”学习发生的地方。它们提取特征，将数据变换到新的空间。
3.  **输出层 (Output Layer)**: 给出最终预测（概率分布、数值等）。

## 3. 激活函数：非线性的魔法

如果没有激活函数，无论网络叠加多少层，本质上都只是一个线性变换（矩阵乘法的组合还是矩阵乘法）。
**激活函数引入了非线性**，让网络能拟合复杂的曲线。

*   **ReLU**: $f(x) = max(0, x)$。简单高效，目前最常用。
*   **Sigmoid**: $f(x) = \\frac{1}{1+e^{-x}}$。将输出压缩到 (0,1)。
*   **Tanh**: 将输出压缩到 (-1, 1)。

\`\`\`quiz
{
  "question": "为什么我们需要激活函数 (Activation Function)？",
  "options": [
    "为了让计算更快",
    "为了引入非线性，使网络能学习复杂的模式",
    "为了防止过拟合",
    "为了将输出限制在 0 和 1 之间"
  ],
  "answer": 1,
  "explanation": "如果没有激活函数，多层神经网络等价于单层线性网络，无法处理非线性问题（如图像识别、语言理解）。"
}
\`\`\`

---
**交互练习**：在下方的可视化面板中，你可以看到信号是如何在层级之间传递的。尝试调整网络的复杂度和速度。
`.trim(),
        visualizerType: 'neural-net'
      },
      {
        id: 'bp-02',
        title: '反向传播 (Backpropagation)',
        description: '网络是如何"学习"的？链式法则的应用。',
        content: `
# 反向传播：学习的引擎

如果说前向传播是“推理”，那么反向传播 (Backpropagation) 就是“反思”和“学习”。

## 1. 核心流程

1.  **前向传播 (Forward Pass)**: 数据输入网络，经过层层计算，得到预测值 $\\hat{y}$。
2.  **计算损失**: 比较 $\\hat{y}$ 和真实值 $y$，算出误差 $L$。
3.  **反向传播 (Backward Pass)**: 这是一个“归咎”的过程。我们想知道：**每个权重 $w$ 对这个误差 $L$ 贡献了多少？**

## 2. 链式法则 (Chain Rule)

微积分中的链式法则是反向传播的数学基础。

假设 $L = f(y)$, $y = g(x)$, $x = h(w)$。
那么 $L$ 对 $w$ 的导数是：

$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} \\cdot \\frac{\\partial x}{\\partial w} $$

## 3. 参数更新

一旦我们要到了梯度 $\\nabla w$，我们就用它来更新参数：

$$ w \\leftarrow w - \\text{learning\\_rate} \\times \\nabla w $$

通过成千上万次的迭代，网络逐渐“学会”了任务。
`.trim(),
        visualizerType: 'loss-chart'
      }
    ]
  },
  {
    id: 'ai-adv',
    title: 'AI 前沿进阶',
    description: '涵盖CV、NLP、大模型与具身智能，探索人工智能的最前沿。',
    icon: 'Bot',
    color: 'from-amber-400 to-orange-600',
    difficulty: Difficulty.Advanced,
    lessons: [
      {
        id: 'cv-01',
        title: '计算机视觉 (CV)',
        description: '从CNN到Vision Transformers，让机器看懂世界。',
        content: `
# 计算机视觉：机器之眼

计算机视觉 (Computer Vision) 致力于让机器“看”见并理解图像与视频。

## 1. 卷积神经网络 (CNN)

CNN 是视觉领域的王者。它的核心在于**卷积 (Convolution)** 和 **池化 (Pooling)**。

- **局部感知**: 卷积核 (Kernel) 就像一个滑动窗口，扫描图像，提取边缘、纹理等局部特征。
- **参数共享**: 同一个卷积核在整张图上使用，大大减少了参数量。
- **层次化特征**: 浅层网络提取边缘，深层网络提取眼睛、轮子等复杂形状。

## 2. Vision Transformers (ViT)

近年来，Transformer 架构跨界来到了视觉领域。
ViT 将图像切割成一系列 16x16 的小块 (Patches)，将它们视为“单词”，利用 Self-Attention 机制捕捉全局联系。

## 应用场景
- **目标检测 (Object Detection)**: 比如 YOLO 系列。
- **语义分割 (Semantic Segmentation)**: 给每个像素打标签。
- **生成对抗网络 (GANs)**: 创造不存在的人脸或艺术画。

---
**交互练习**：下方的可视化展示了**卷积操作 (Convolution)** 的过程。观察 3x3 的卷积核（Kernel）如何在 5x5 的输入图像上滑动，通过点积运算提取特征（Feature Map）。
`.trim(),
        visualizerType: 'cnn-viz'
      },
      {
        id: 'nlp-llm-02',
        title: 'NLP 与 大语言模型 (LLM)',
        description: 'Transformer架构解析与GPT原理。',
        content: `
# NLP 与 LLM：语言的革命

自然语言处理 (NLP) 经历了从规则、统计模型 (RNN/LSTM) 到 Transformer 的革命性跨越。

## 1. Transformer：Attention is All You Need

2017年 Google 提出的 Transformer 架构改变了一切。
它抛弃了循环结构，完全依赖 **Attention (注意力)** 机制。

$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$

这也允许了模型进行**大规模并行训练**。

## 2. BERT 与 GPT

- **BERT (Encoder-only)**: 擅长理解。使用“完形填空”的方式预训练。
- **GPT (Decoder-only)**: 擅长生成。使用“预测下一个词”的方式预训练。

## 3. 大语言模型 (LLM)

当模型参数量达到百亿/千亿级别 (Scaling Law)，神奇的**涌现 (Emergence)** 现象发生了：
- **In-Context Learning**: 不需要微调，只看几个例子就能学会新任务。
- **思维链 (Chain of Thought)**: 能进行多步逻辑推理。
`.trim(),
        visualizerType: 'none'
      },
      {
        id: 'embodied-03',
        title: '具身智能 (Embodied AI)',
        description: 'AI大脑 + 物理身体：迈向通用机器人的关键。',
        content: `
# 具身智能：AI 走进物理世界

如果 ChatGPT 是“缸中之脑”，那么具身智能 (Embodied AI) 就是给这个大脑装上身体（传感器和执行器）。

## 1. 核心循环

$$ 感知 \\rightarrow 规划 \\rightarrow 控制 $$

1.  **感知 (Perception)**: 多模态输入（视觉 RGB-D、触觉、听觉、IMU）。
2.  **规划 (Planning)**: 结合 LLM 的世界知识，将高层指令（"去厨房拿个苹果"）分解为子任务。
3.  **控制 (Control)**: 将动作指令转化为关节电机的电压或力矩。

## 2. 挑战：Sim2Real Gap

在仿真器 (Simulator) 里训练机器人既安全又快速（可以快进时间）。
但现实世界充满了摩擦、形变、光照变化等不可预测因素。
如何将仿真中学会的策略迁移到真机，是 **Sim2Real** 的核心难题。

## 3. 端到端 (End-to-End)

传统的机器人控制由多个模块串联。
现在的趋势是训练一个**通用的 Robot Foundation Model**，直接从像素映射到动作 (Pixels to Actions)。
`.trim(),
        visualizerType: 'none'
      }
    ]
  }
];
